{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Analytics und Künstliche Intelligenz\n",
    "Wintersemester 2023/2024\n",
    "\n",
    "Prof. Dr. Jürgen Bock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lernziele\n",
    "* Sie sind in der Lage den Unterschied zwischen Modellparametern und Hyperparametern zu erläutern und einige Hyperparameter aufzuzählen.\n",
    "* Sie sind in der Lage wesentliche Schritte beim Vorbereiten von Datensätzen darzustellen, und können mehrere Methoden anwenden um sich einen Überblick über einen Datensatz zu verschaffen.\n",
    "* Sie können Herangehensweisen beim Kodieren der Ausagbe von Multi-Class Klassifikatornetzen skizzieren, können das Prinzip geeigneter Aktivierungsfunktionen darstellen, und sind in der Lage die Ausgaben dieser Aktivierungsfunktionen zu interpretieren.\n",
    "* Sie sind in der Lage neuronale Netze zur Multi-Class Klassifikation anzuwenden und die Klassifikationsergebnisse anhand geegneter Metriken zu evaluieren.\n",
    "* Sie können das Grundprinzip von Convolutional Neural Networks zur Bildklassifikation wiedergeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter von Neuronalen Netzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es werden grundsätzlich zwei Arten von Parametern unterschieden:\n",
    "- Modellparameter\n",
    "- Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modellparameter** sind die Parameter, die das eigentliche (gelernte) Modell ausmachen. Im Falle von Neuronalen Netzen sind dies die **Gewichte**. Wurde bspw. das neuronale Netz traininert um Katzen auf Bildern zu erkennen, so charakterisieren die Modellparameter (Gewichte) das Modell von Katzenbildern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter** sind die Paremeter, die die Struktur des Modells und des Trainingsverfahrens charakterisieren. Alle Stellschrauben an denen man beim Trainineren drehen kann sind Hyperparameter. Dazu zählen:\n",
    "- Struktur des neuronalen Netzes\n",
    "  - Anzahl der Layers\n",
    "  - Anzahl der Neuronen pro Layer\n",
    "  - Art der Vernetzung (fully connected, convolutional, etc.)\n",
    "- Anzahl der Trainingsepochen\n",
    "- Stapelgröße im Batchtraining\n",
    "- Wahl der *loss function*\n",
    "- Wahl des Optimierers\n",
    "- Learning Rate\n",
    "- Codierung des Ergebnisses im Ausgangsvektor\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wahl der Hyperparameter ist eine der schwierigsten Aufgaben beim Training von neuronalen Netzen. Oftmals sind die Auswirkungen bestimmter Hyperparameter nicht systematisch zu bestimmen. Des weiteren sind Wechselwirkungen zwischen den Hyperparametern möglich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Künstliche Neuronale Netze zur Bilderkennung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden uns noch ein paar Hyperparameter genauer anschauen, indem wir und mit einer prominenten Anwendung von neuronalen Netzen beschäftigen: Der Bilderkennung.\n",
    "\n",
    "Genauer gesagt handelt es sich dabei um die Klassifikation des Bildinhalts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bilddaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilddaten sind deutlich komplexer als die bisher betrachteten künstlich generierten Daten, oder die einfachen Klassifikationsdatensätze, wie *iris*.\n",
    "\n",
    "In klassischen Bilderkennungsverfahren wurde in einem Vorverarbeitungsschritt eine gewisse Menge an Bildmerkmalen detektiert und extrahiert. (Kanten, Ecken, etc.)\n",
    "\n",
    "In neuronalen Netzen wird jedes Pixel des Bildes als Merkmal betrachtet. Entsprechend groß ist der Eingangsvektor des neuronalen Netzes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispielsweise für ein Bild der Größe 32 x 32 Pixel in 3 Farbkanälen benötigen wir einen Eingangsvektor der Größe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Möglichkeit wurde erst durch die jüngsten Fortschritte im Bereich neuronaler Netze geschaffen:\n",
    "- Verfügbarkeit von großen Datenmengen\n",
    "- Verfügbarkeit von performanter Rechenleistung\n",
    "- Neuartige / verbesserte Algorithmen\n",
    "\n",
    "Durch diese Fortschritte ist es möglich, sehr große neuronale Netze mit vielen Schichten mit einer Vielzahl von Neuronen - und auch entsprechend großer Eingangsvektoren - zu definieren und zu trainieren. Das Trainieren und Auswerten von Modellen mit solchen vielschichtigen Architekturen wird als *Deep Learning* bezeichnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorbereiten und Bereitstellen von Bilddaten mit PyTorch / Torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die `torchvision` Bibliothek enthält verschiedene nützliche Pakete und Module zum Laden, Verwalten und Manipulieren von Bilddaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Paket `datasets` in ``torchvision`` enthält Module zum Laden von frei verfügbaren und häufig (z.B. für Benchmarks) verwendeten Bilddatensätzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Paket `transforms` enthält Klassen zur Transformation von Bilddaten (z.B. Umwandlung in Tensoren, Umskalierung, Normalisierung, Zuschneiden, ...). Transformationen können einerseits verwendet werden, um die Bilder in ein für das neuronale Netz verwendbares Format zu konvertieren. Andererseits lassen sich damit Trainingsdaten erweitern, indem z.B. verschiedene Bildanschnitte verwendet werden, Bilder gedreht, gespiegelt, verzerrt, mit verschiedenen Sättigungs- / Kontrast- / Helligkeitsveränderungen modifiziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Transformationen können direkt der Repräsentation des Datensatzes übergeben werden. Sie werden dann vom Datensatzobjekt angewendet.\n",
    "\n",
    "Zunächst muss jedoch ein Transformationsobjekt konfiguriert und instanziiert werden. Die Klasse `Compose` übernimmt bei der Instanziierung eine Liste von `transform` Objekten, die der Reihe nach ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchvision` kann bekannte Datensätze bei Bedarf direkt herunterladen und in einem angegebenen Verzeichnis abspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_cifar100 = \"c:/data/cifar100/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sind die Daten am angegebenen Ort bereits vorhanden, wird der Download übersprüngen.\n",
    "\n",
    "Je nach Datensatz stehen verschiedene Möglichkeiten zur Verfügung, z.B. wenn der Datensatz Test- und Trainingssätze enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to c:/data/cifar100/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_cifar100_train \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCIFAR100\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_dir_cifar100\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bacco\\Desktop\\Master\\WS2023\\baki\\BAKI_WS2023\\.venv\\lib\\site-packages\\torchvision\\datasets\\cifar.py:65\u001b[0m, in \u001b[0;36mCIFAR10.__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain \u001b[38;5;241m=\u001b[39m train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bacco\\Desktop\\Master\\WS2023\\baki\\BAKI_WS2023\\.venv\\lib\\site-packages\\torchvision\\datasets\\cifar.py:139\u001b[0m, in \u001b[0;36mCIFAR10.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles already downloaded and verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bacco\\Desktop\\Master\\WS2023\\baki\\BAKI_WS2023\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:434\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[0;32m    432\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 434\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bacco\\Desktop\\Master\\WS2023\\baki\\BAKI_WS2023\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:144\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m url \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fpath)\n\u001b[1;32m--> 144\u001b[0m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\bacco\\Desktop\\Master\\WS2023\\baki\\BAKI_WS2023\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:48\u001b[0m, in \u001b[0;36m_urlretrieve\u001b[1;34m(url, filename, chunk_size)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_urlretrieve\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m, chunk_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m---> 48\u001b[0m         \u001b[43m_save_response_content\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bacco\\Desktop\\Master\\WS2023\\baki\\BAKI_WS2023\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:37\u001b[0m, in \u001b[0;36m_save_response_content\u001b[1;34m(content, destination, length)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save_response_content\u001b[39m(\n\u001b[0;32m     32\u001b[0m     content: Iterator[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m     33\u001b[0m     destination: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     34\u001b[0m     length: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(destination, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total\u001b[38;5;241m=\u001b[39mlength) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m---> 37\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m content:\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunk:\n\u001b[0;32m     40\u001b[0m                 \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bacco\\Desktop\\Master\\WS2023\\baki\\BAKI_WS2023\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:48\u001b[0m, in \u001b[0;36m_urlretrieve.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_urlretrieve\u001b[39m(url: \u001b[38;5;28mstr\u001b[39m, filename: \u001b[38;5;28mstr\u001b[39m, chunk_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m---> 48\u001b[0m         _save_response_content(\u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m), filename, length\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mlength)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_cifar100_train = datasets.CIFAR100( \n",
    "    root=root_dir_cifar100,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cifar100_test = datasets.CIFAR100( \n",
    "    root=root_dir_cifar100,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus Lesbarkeitsgründen verwenden wir kürzere Bezeichner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = dataset_cifar100_train\n",
    "data_test = dataset_cifar100_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspektion der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist immer sinnvoll, eine erste manuelle Inspektion der Daten vorzunehmen. Dadurch können wir prüfen, ob die Daten im richtigen Format vorliegen, ob sie gelesen und weiterverarbeitet werden können, und wie ggf. bestimmte Hyperparameter eingestellt werden müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handelt es sich um einen Trainings- oder Testdatensatz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_train ist ein Trainingsdatensatz:', data_train.train)\n",
    "print('data_test ist ein Trainingsdatensatz:', data_test.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In welcher Form liegen die Daten vor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape: ', data_train.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das bedeutet: 50000 Datensamples der Größe 32 x 32 x 3 (d.h. 32 x 32 Pixel in 3 Farbkanälen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wieviele und welche Klassen sind vorhanden?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Anzahl der Klassen: \", len(data_train.classes))\n",
    "\n",
    "for i in range(0, len(data_train.classes)):\n",
    "    print(\"{:2d}  {}\".format(i, data_train.classes[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wieviele Datensamples gibt es?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of samples in training set:\", len(data_train.data))\n",
    "print(\"Number of samples in test set: \", len(data_test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtern und Laden der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Zugriff auf die Daten erfolgt, wie wir bereits wissen, über den `DataLoader` der im `pytorch` Paket `torch.utils.data` bereitgestellt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unserem Beispiel wollen wir nicht den gesamten Datensatz betrachten, sondern nur einen Teil, der bestimmte Target-Klassen enthält. Dazu können wir einen sogenannten `Sampler` konfigureren, der nur eine bestimmte Teilmenge des Datensatzes herausfiltert. Dem Sampler müssen die Indizes der in Frage kommenden Datensamples bekannt gemacht werden. Dazu müssen wir die Indizes dieser Datensamples der entsprechenden Target-Klassen zunächst herausfinden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wählen die gewünschten Klassen anhand ihrer Bezeichnungen aus und ermitteln die Klassenindizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_selection = ['apple', 'pear', 'orange', 'mushroom', 'sweet_pepper']\n",
    "class_selection_idx = [i for i in range(len(data_train.classes)) if data_train.classes[i] in class_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Indizes der Klassen\", class_selection, \":\", class_selection_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir benötigen nun die Indizes der Datensamples, welche mit den ausgewählten Klassen gelabelt sind (für welche das Target einer der gewählten Klassenindizes ist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspizieren wir dazu zunächst den gesamten Target-Vektor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir benötigen die Indizes des Target-Vektors, an denen er eine der ausgewählten Klassen benennt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_idx = [i for (i,t) in enumerate(data_train.targets) if t in class_selection_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier sind die Indizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und hier zur Kontrolle der Teil-Target-Vektor selbst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([data_train.targets[i] for i in target_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir mit Hilfe der Indexliste den `Sampler` erstellen. Die Samplerklassen finden sich im `torch.utils.data.sampler` Package. Wir verwenden den `SubsetRandomSampler` auf Basis der Indizes, die das Subset charakterisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampler = SubsetRandomSampler(target_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der eigentliche `DataLoader` kann nun für das `dataset` unter Angabe der `batch_size` und des `sampler`s erstellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset=data_train, batch_size=50, sampler=data_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem ``DataLoader`` können wir jetzt erstmals über die Bilder iterieren und einen Blick darauf werfen. Da wir unseren ``SubsetRandomSampler`` verwenden, sehen wir nur Bilder der ausgewählten Klassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im bereitgestellten ``dataview`` Modul - dieses sollte im selben Verzeichnis wie dieses Notebook liegen - gibt es eine Hilfsfunktion zum Anzeigen von Bildern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir zeigen die Bilder *batch*-weise an, wie wir sie vom ``DataLoader`` bekommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (input, _) in data_loader:\n",
    "    dataview.view_images(input, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Neuronale Netz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eingangs- und Ausgangsvektor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir definieren zunächst die Länge des Eingangsvektors: (32x32 Pixel in 3 Farbkanälen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 32 * 32 * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bisher hatten wir neuronale Netze nur zur Binärklassifikation betrachtet. Dabei bestand die letzte Schicht aus genau einem Neuron, welches (bedingt durch die *sigmoid* oder *threshold* Aktivierungsfunktion Werte (nahe) 0 und (nahe) 1 ausgibt, was den beiden Klassen entspricht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Beispiel haben wir es mit einer Multiclass Klassifikation zu tun, d.h. es gibt mehr als 2 Klassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir benötigen eine Methode um die verschiedenen Klassen als Ausgabe des neuronalen Netzes zu repräsentieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt zwei typische Ansätze, um Klassen zu kodieren:\n",
    "- Label Encoding\n",
    "- One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim **Label Encoding** wird jede Klasse durch eine eindeutigen Klassenindex repräsentiert. In unserem Falle wäre dies z.B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Klasse       | Label |\n",
    "|--------------|-------|\n",
    "| Apple        | 0     |\n",
    "| Pear         | 1     |\n",
    "| Orange       | 2     |\n",
    "| Mushroom     | 3     |\n",
    "| Sweet_Pepper | 4     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Vorteil ist die kompakte Repräsentation der Klasse. Ein Nachteil ist, dass durch die aufsteigende Nummerierung eine Reihenfolge suggeriert wird, die nicht vorhanden ist. Insbesondere, wenn das Klassenlabel später als numerischer Eingang zur Weiterverarbeitung verwendet wird könnte der Zahlenwert missinterpretiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim **One-Hot Encoding** wird für $n$ Klassen ein $n$-elementiger Vektor zur Repräsentation verwendet. In diesem Vektor gibt der Index die jeweilige Klasse an. Nach eindeutiger Klassifikation ist der Vektor in allen Elementen 0, außer in dem Element mit Index der erkannten Klasse ist er 1. In unserem Beispiel wäre das"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Klasse       | Apple | Pear | Orange | Mushroom | Sweet_Pepper |\n",
    "|--------------|-------|------|--------|----------|--------------|\n",
    "| Apple        | 1     | 0    | 0      | 0        | 0            |\n",
    "| Pear         | 0     | 1    | 0      | 0        | 0            |\n",
    "| Orange       | 0     | 0    | 1      | 0        | 0            |\n",
    "| Mushroom     | 0     | 0    | 0      | 1        | 0            |\n",
    "| Sweet_Pepper | 0     | 0    | 0      | 0        | 1            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Verwendung des One-Hot Encodings für die Repräsentation des Ergebnisvektors neuronaler Netze bei Multiclass Klassifikation besitzt einen entscheidenden Vorteil gegenüber des Label Encodings: Das neuronale Netz liefert in der Regel keine eindeutige Klassenzuordnung mit einem Vektorelement gleich 1 und allen anderen gleich 0. Vielmehr werden die entsprechenden Ausgangsneuronen unterschiedlich stark aktiviert. Je stärker eines dieser Neuronen aktiviert ist, umso größer die Wahrscheinlichkeit für die entsprechende Klasse. (Eine Ergebnisdarstellung, die sich durch das Label Encoding nicht realisieren lässt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für Multiclass Klassifikation gilt es also zunächst ein neuronales Netz zu konstruieren dessen *output layer* soviele Neuronen wie Zielklassen aufweist. Um die Aktivierungen in eine entsprechende Wahrscheinlichkeitsverteilung umzuwandeln  kann die *Softmax* Aktivierungsfunktion verwendet werden. Diese konvertiert einen Vektor $\\vec{a}$ von Aktivierungen in einen gleichgroßen Vektor von Wahrscheinlichkeiten $\\vec{y}$, so dass für jedes Vektorelement $y_i$ gilt $y_i \\in [0, 1]$ und $\\sum_i y_i = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"softmax.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1, 5)\n",
    "print('Aktivierungen a:                      ', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "print('Wahrscheinlichkeiten nach softmax(a): ', softmax(a))\n",
    "print('Summe der Wahrscheinlichkeiten:       ', torch.sum(softmax(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine häufige Verwendung findet auch die *LogSoftmax* Aktivierungsfunktion. Diese logarithmiert die Ergebnisse der *Softmax*-Funktion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die *LogSoftmax*-Funktion liefert Werte im Bereich $(-\\infty, 0]$. Dies ist durch die Logarithmus-Funktion zu erklären:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "xdata = torch.linspace(0, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xdata, torch.log(xdata))\n",
    "plt.grid(True)\n",
    "plt.axvline(1, color=\"red\", linestyle=\"--\")\n",
    "plt.axvline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dadurch werden höhere Wahrscheinlichkeiten (nahe 1) stärker betont (nach der Logarithmierung nahe 0) und geringe Wahrscheinlichkeiten (nahe 0) weiter abgeschwächt (nach der Logarithmierung nahe $-\\infty$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsoftmax = nn.LogSoftmax(dim=1)\n",
    "print('Aktivierungen a:                      ', a)\n",
    "print('Wahrscheinlichkeiten nach softmax(a): ', softmax(a))\n",
    "print('Aktivierungen nach logsoftmax(a):     ', logsoftmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(torch.arange(len(a.flatten())) -0.3, a.flatten(), 0.2, label=\"Aktivierung\")\n",
    "plt.bar(torch.arange(len(a.flatten())) -0.1, softmax(a).flatten(), 0.2, label=\"Softmax\")\n",
    "plt.bar(torch.arange(len(a.flatten())) +0.1, logsoftmax(a).flatten(), 0.2, label=\"LogSoftmax\")\n",
    "plt.title(\"Vergleich Aktivierungsfunktionen\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ob mit *Softmax* oder *LogSoftmax* Aktivierung - die Länge des Ausgangsvektors des neuronalen Netzes entspricht der Anzahl der Zielklassen. In unserem Beispiel ist dies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_output = len(class_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somit haben wir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_input)\n",
    "print(n_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Netzstruktur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir definieren zunächst ein klassisches *Multi-Layer Perceptron* wie bekannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_input, 200)\n",
    "        self.fc2 = nn.Linear(200, 50)\n",
    "        self.fc3 = nn.Linear(50, n_output)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = input.view(-1, n_input)\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanziierung des Modells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimierer und *loss function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden den performanten Optimierungsalgorithmus *Adam* zur Anpassung der Gewichte (Modellparameter) mit entsprechender *learning rate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als *loss function* für Multiclass Klassifikation eignet sich die *CrossEntropyLoss*, analog zur *BinaryCrossEntropy* für binäre Klassifikation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die ``nn.CrossEntropyLoss`` Funktion hat eine wichtige Besonderheit:\n",
    "\n",
    "Sie erwartet als Argumente die Vorhersage des Neuronalen Netzes und die Zielklasse, um den Fehler zu berechnen.\n",
    "\n",
    "Dabei ist das Ergebnis des neuronalen Netzes als unnormalisierter Vektor bestehend aus den Bewertungen für die einzelnen Klassen einzubringen. Die Normalisierung entsprechend der *LogSoftmax* Aktivierungsfunktion ist dabei Bestandteil der *CrossEntropyLoss* Funktion und muss (in der Trainingsphase) vom *forward pass* des neuronalen Netzes ausgenommen werden.\n",
    "\n",
    "Die erwartete Zielklasse wird als ein skalarer Wert übergeben, der dem Klassenindex entspricht.\n",
    "\n",
    "Der Klassenindex ist dabei ein Wert im Bereich $[0, ..., numberClasses - 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unserem speziellen Fall müssen wir also die Klassenindizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_selection_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "umwandeln in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(range(len(class_selection_idx))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beispiel:** Der Ausgangsvektor des neuronalen Netzes sei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.tensor([[3.254, 0.252, 0.542, 6.233, 1.042]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Target-Klassenindex ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die *CrossEntropyLoss* berechnet sich zu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_fn(out, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Traininsschleife muss dies entsprechend stapelweise geschehen, also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.tensor([[3.253, 6.124, 0.346, 0.446, 1.153],\n",
    "                    [0.421, 5.255, 1.155, 0.421, 9.532],\n",
    "                    [0.221, 0.564, 1.435, 2.351, 0.532]])\n",
    "t = torch.tensor([1, 4, 3])\n",
    "print(loss_fn(out, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainings-Schleife"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Visualisierung während dem Durchlauf der Trainingsschleife benötigen wir einige Hilfsobjekte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from statistics import mean\n",
    "loss_history = []\n",
    "loss_ep = []\n",
    "plt.figure(figsize = (12,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Traininsschleife iteriert wie gehabt über verschiedene Epochen und innerhalb jeder Epoche über die durch den ``DataLoader`` angelieferten Stapel (*batches*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entsprechend der oben erläuterten Besonderheiten der *CrossEntropyLoss* Funktion muss das Target-Argument in jeder Schleifeniteration aufbereitet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs) :\n",
    "    for b, batch in enumerate(data_loader) :\n",
    "        optimizer.zero_grad()\n",
    "        input, target = batch\n",
    "        output = model(input)\n",
    "        # The target returned by the DataLoader is a tensor with the original class labels\n",
    "        # For the CrossEntropyLoss function we need to map this to a 1D tensor with each element \n",
    "        # a class index in [0, ..., number_of_classes-1]\n",
    "        t = torch.LongTensor(len(target))   # len(target) corresponds to the batch size\n",
    "        for i, e in enumerate(target):\n",
    "            t[i] = torch.tensor(class_selection_idx.index(e.item()))\n",
    "        loss = loss_fn(output, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ep.append(loss.item())   \n",
    "        \n",
    "    ## Zu Visualisierungszwecken:\n",
    "    loss_history.append(mean(loss_ep))\n",
    "    loss_ep = []\n",
    "    display.clear_output(wait=True)\n",
    "    plt.plot(loss_history)\n",
    "    display.display(plt.gcf())\n",
    "    display.display(print(\"Epoch {:2}, loss: {}\".format(epoch, loss_history[-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Evaluation des trainierten Modells ziehen wir den Test-Datensatz heran."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diesen bereiten wir analog zu dem Trainings-Datensatz für die Verwendung auf.\n",
    "\n",
    "Extrahieren der Datensamples mit ausgewählten Klassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target_idx = [i for (i,t) in enumerate(data_test.targets) if t in class_selection_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dazu benötigen wir einen entsprechenden ``SubsetRandomSampler`` ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampler_test = SubsetRandomSampler(test_target_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... und einen entsprechenden ``DataLoader``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_test = DataLoader(dataset=data_test, batch_size=10, sampler=data_sampler_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Evaluation verwenden wir die klassischen Metriken aus ``scikit-learn``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir erstellen zwei Listen:\n",
    "- ``y_test`` zur Auflistung der Zielklassen aus dem Testdatensatz (erwartete Klassifizierung)\n",
    "- ``y_pred`` zur Auflistung der jeweiligen Berechnung des neuronalen Netzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Training hatten wir die *LogSoftmax* Aktivierung wegen der *CrossEntropyLoss* Funktion weggelassen. Wollten wir die Wahrscheinlichkeiten der berechneten Klassenzugehörigkeiten erfahren, müssten wir die *Softmax* Funktion hier auf die Ausgabe des Modells anwenden.\n",
    "\n",
    "Da sich die Reihenfolge, also die \"Rangliste\", der Klassenzugehörigkeitswahrscheinlichkeiten dadurch nicht ändert, können wir auch die Klasse mit der höchsten Bewertung ermitteln. Dies geschieht durch die ``argmax`` Funktion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.randn(1,5)\n",
    "print(l)\n",
    "print(l.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Gewinnerklassen des Modells und die Zielklassen speichern wir also in den zuvor initialisierten Listen ``y_pred`` und ``y_test``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_test in data_loader_test:\n",
    "    input, target = batch_test\n",
    "\n",
    "    for t in target:\n",
    "        y_test.append(class_selection_idx.index(t.item()))\n",
    "\n",
    "    prediction = model(input)\n",
    "    \n",
    "    for y in prediction:\n",
    "        y_pred.append(y.argmax().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die beiden Listen können wir nun zur Berechnung der *Confusion Matrix* und des *Classification Report*s heranziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:\\n', confusion)\n",
    "print('\\n\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred, target_names=class_selection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausblick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den bisherigen neuronalen Netzen haben wir das Bild in einen langen eindimensionalen Vektor konvertiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In einem Bild befinden sich Merkmale jedoch häufig in einem Bereich benachbarter Pixel, und zwar in zwei Dimensionen. Diese Tatsache nutzen Convolutional Neural Networks (CNNs), indem sie das Bild in seiner ursprünglichen Dimensionalität als Eingabe annehmen. In einer Reihe von *feature detection layers* werden benachbarte Pixelgruppen mit Filtern verrechnet (*convolution*) und komprimiert (*pooling*). Im Anschluss finden typischerweise mehrere *fully connected layers* als Klassifikator Anwendung. Dieser führt auf Grundlage der komprimierten Merkmale die Klassifizierung durch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ein Beispiel wie ein Convolutional Neural Network mit PyTorch definiert werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, kernel_size=6, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(12, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool2d((6, 6))\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64*6*6, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, n_output)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.features(input)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach Änderung des Modells muss der Optimierer mit den neuen Modellparametern vertraut gemacht werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für eine weitreichende Erläuterung des Prinzips betrachten Sie bitte die einschlägige Literatur und die PyTorch Dokumentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
