{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Business Analytics und Künstliche Intelligenz\n",
    "\n",
    "Prof. Dr. Jürgen Bock & Maximilian-Peter Radtke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellbewertung auf Basis eines Validierungssets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Modellbewertung auf Basis eines Validierungssets, benutzen wir die Funktion `train_test_split` von Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu zeigen wie dies funktioniert und welche Nachteile dieser Ansatz haben kann, führen wir eine Klassifikation auf dem Auto-Datensatz aus bezüglich der `Origin` Spalte aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv('Auto_clean.csv')\n",
    "auto['intercept'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mithilfe von `train_test_split` können wir die Daten in ein Trainingsset und ein Validierungsset / Testset unterteilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = auto.drop(['year', 'origin', 'name'], axis=1)\n",
    "y = auto.origin\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir nutzen den LDA Klassifikator, um die Herkunft der Autos auf Basis der Daten vorherzusagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaAuto = LinearDiscriminantAnalysis()\n",
    "ldaAuto.fit(XTrain, yTrain)\n",
    "print('Trainingsfehler ', 1-ldaAuto.score(XTrain, yTrain))\n",
    "print('Testfehler: ', 1-ldaAuto.score(XTest, yTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Testfehler ist nur minimal größer als der Trainingsfehler. Allerdings kann diese Ansicht irreführend sein, da die Aufteilung zwischen Trainingsset und Validierungsset zufällig ist. Mit dem  `random_state`-Parameter lässt sich festlegen welche Mischung der Daten jeweils vorgenommen wird. Wenn uns über verschiedene Random States die Fehler ausgeben lassen, bekommen wir verschiedenen Fehler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    ldaAuto = LinearDiscriminantAnalysis()\n",
    "    ldaAuto.fit(XTrain, yTrain)\n",
    "    print('Random State:', i)\n",
    "    print('Trainingsfehler:', 1-ldaAuto.score(XTrain, yTrain))\n",
    "    print('Testfehler:', 1-ldaAuto.score(XTest, yTest))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Weg diese Streuung zu umgehen, ist der in der Vorlesung besprochenen Cross-Validation Ansatz. Hierzu importieren wir `KFold` aus scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits, random_state=1, shuffle=True)\n",
    "CVFoldTestErrors = []\n",
    "CVFoldTrainErrors = []\n",
    "for trainIndex, testIndex in kf.split(X):\n",
    "    XTrain, XTest = X.iloc[trainIndex], X.iloc[testIndex]\n",
    "    yTrain, yTest = y.iloc[trainIndex], y.iloc[testIndex]\n",
    "    ldaAuto = LinearDiscriminantAnalysis()\n",
    "    ldaAuto.fit(XTrain, yTrain)\n",
    "    CVFoldTrainErrors.append(1-ldaAuto.score(XTrain, yTrain))\n",
    "    CVFoldTestErrors.append(1-ldaAuto.score(XTest, yTest))\n",
    "print('CV Trainingsfehler:\\n', CVFoldTrainErrors)\n",
    "print('CV Durchschnittlicher Trainingsfehler:', np.mean(CVFoldTrainErrors))\n",
    "print('CV Testfehler:\\n', CVFoldTestErrors)\n",
    "print('CV Durchschnittlicher Testfehler:', np.mean(CVFoldTestErrors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trotzdem gibt es eine gewisse Streuung zwischen den Ergebnissen, da auch hier zufällig die Splits ausgewählt werden. Wenn wir uns wieder über verschiedene Random States die Ergebnisse ausgeben lassen, erhalten wir das folgende Ergebnis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "for i in range(10):\n",
    "    kf = KFold(n_splits, random_state=i, shuffle=True)\n",
    "    CVFoldTestErrors = []\n",
    "    CVFoldTrainErrors = []\n",
    "    for trainIndex, testIndex in kf.split(X):\n",
    "        XTrain, XTest = X.iloc[trainIndex], X.iloc[testIndex]\n",
    "        yTrain, yTest = y.iloc[trainIndex], y.iloc[testIndex]\n",
    "        ldaAuto = LinearDiscriminantAnalysis()\n",
    "        ldaAuto.fit(XTrain, yTrain)\n",
    "        CVFoldTrainErrors.append(1-ldaAuto.score(XTrain, yTrain))\n",
    "        CVFoldTestErrors.append(1-ldaAuto.score(XTest, yTest))\n",
    "    print('Random State:', i)\n",
    "    print('CV Durchschnittlicher Trainingsfehler:', np.mean(CVFoldTrainErrors))\n",
    "    print('CV Durchschnittlicher Testfehler:', np.mean(CVFoldTestErrors))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um überhaupt keine Streuung in den Ergebnissen zu haben, können wir Leave One Out Cross Validation nutzen. Dazu setzen wir die Anazhl der Splits auf die Anzahl der Zeilen in dem Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_splits = X.shape[0]\n",
    "kf = KFold(n_splits, random_state=1, shuffle=True)\n",
    "CVFoldTestErrors = []\n",
    "CVFoldTrainErrors = []\n",
    "for trainIndex, testIndex in kf.split(X):\n",
    "    XTrain, XTest = X.iloc[trainIndex], X.iloc[testIndex]\n",
    "    yTrain, yTest = y.iloc[trainIndex], y.iloc[testIndex]\n",
    "    ldaAuto = LinearDiscriminantAnalysis()\n",
    "    ldaAuto.fit(XTrain, yTrain)\n",
    "    CVFoldTrainErrors.append(1-ldaAuto.score(XTrain, yTrain))\n",
    "    CVFoldTestErrors.append(1-ldaAuto.score(XTest, yTest))\n",
    "print('LOOCV Durchschnittlicher Trainingsfehler:', np.mean(CVFoldTrainErrors))\n",
    "print('LOOCV Durchschnittlicher Testfehler:', np.mean(CVFoldTestErrors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der `KFold`-Methode haben wir die Möglichkeit jegliche Art von Modell zu nutzen und genau zu bestimmen was mit den einzelnen Splits passiert. Falls wir aber einen Algorithmus von scikit-learn direkt nutzen, lässt sich Cross-Validation auch einfacher umsetzen. Hierzu müssen wir nur `cross_validate` importieren und anwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaAuto = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(ldaAuto, X, y, cv=5, return_train_score=True)\n",
    "print('Trainingsfehler:', 1-scores['train_score'])\n",
    "print('Testfehler:', 1-scores['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data leakage\n",
    "\n",
    "Data leakage tritt auf, wenn Informationen aus dem Testdatensatz unbeabsichtigt den Trainingsprozess beeinflussen, was zu überoptimistischen Leistungsabschätzungen führen kann.\n",
    "Dies kann durch unsachgemäße Verwendung von Informationen aus dem gesamten Datensatz während des Trainings oder der Validierung verursacht werden.\n",
    "\n",
    "Im folgenden wollen wir das Beispiel aus der Vorlesung nachbauen, um dies zu veranschaulichen.\n",
    "\n",
    "Zunächst simulieren wir uns hierzu einen Datensatz für ein binäres Klassifikationsprobelm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setze Parameter\n",
    "num_obs = 50 # Anzahl Observationen\n",
    "num_predictors = 5000 # Anzahl Prädiktoren\n",
    "num_selected = 100 # Anzahl an Prädiktoren die genutzt werden sollen\n",
    "num_splits = 5 # Anzahl CV Splits\n",
    "random_seed = 42\n",
    "\n",
    "# Setze random seed um Wiederholbarkeit zu gewährleisten\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# Simuliere Daten für binäres Klassifikationsproblem\n",
    "X = np.random.randn(num_obs, num_predictors)\n",
    "y = np.random.randint(0, 2, num_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Daten sind vollkommen zufällig. Entsprechend gibt es keinen Zusammenhang zwischen $X$ und $y$, d.h. viel besser als der Zufall (Genaugikeit von 50%) sollte unser Modell eigentlich nicht werden. Trotzdem wollen wir es versuchen. Dazu wählen wir die 50 Prädiktoren aus, die am stärksten mit der Zielvariable korrelieren und schätzen basierend auf diesen ein Modell.\n",
    "\n",
    "Im ersten Versuch wählen wir die Prädiktoren vor dem Train/Test Split aus $\\rightarrow$ Data Leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wähle die 100 Prädiktoren mit der stärksten Korrelation mit der Zielvariable aus\n",
    "# Dieser Schritt passiert vor dem Train/Test Split! -> Data leakage\n",
    "selected_predictors = np.argsort(\n",
    "    np.abs(np.corrcoef(X.T, y)[num_predictors][:num_predictors])\n",
    ")[-num_selected:]\n",
    "X_selected = X[:, selected_predictors]\n",
    "\n",
    "# Initialisiere Klassifikationsmodell\n",
    "model = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Evaluiere \"leaky\" Modell mit cross validation\n",
    "scores_leaky = cross_validate(model, X_selected, y, cv=num_splits)['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im zweiten Versuch nehmen wir die Auswahl der besten Prädiktoren mit in unseren Cross Validation Loop $\\rightarrow$ kein Data Leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(num_splits, shuffle=True)\n",
    "CVFoldTestErrors = []\n",
    "# Loop für cross validation\n",
    "for trainIndex, testIndex in kf.split(X):\n",
    "    # Train/Test Split der Daten\n",
    "    X_train, X_test = X[trainIndex, :], X[testIndex, :]\n",
    "    y_train, y_test = y[trainIndex], y[testIndex]\n",
    "\n",
    "    # Wähle die 100 Prädiktoren mit der stärksten Korrelation mit der Zielvariable\n",
    "    # erst nach dem Train/Test split aus\n",
    "    # -> Kein Data leakage\n",
    "    selected_predictors = np.argsort(\n",
    "        np.abs(np.corrcoef(X_train.T, y_train)[num_predictors][:num_predictors])\n",
    "    )[-num_selected:]\n",
    "    X_train_selected = X_train[:, selected_predictors]\n",
    "    X_test_selected = X_test[:, selected_predictors]\n",
    "\n",
    "    # Initialisiere Modell\n",
    "    model = LinearDiscriminantAnalysis()\n",
    "    \n",
    "    # Trainiere Modell auf Basis der Trainingsdaten\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Evaluaiere das Modell auf Basis der Testdaten für spezifischen Fold\n",
    "    CVFoldTestErrors.append(model.score(X_test_selected, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der Testgenauigkeiten\n",
    "print(f'Testgenauigkeit mit data leakage: {np.mean(scores_leaky)}')\n",
    "print(f'Testgenauigkeit ohne data leakage: {np.mean(CVFoldTestErrors)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nutzen Sie auch die logistische Regression und KNN um das Klassifikationsproblem aus der Übung zu lösen. Vergleichen Sie ihre Ergebnisse. Welcher Algorithmus schneidet am besten ab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_validate, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = pd.read_csv('Auto_clean.csv')\n",
    "auto['intercept'] = 1\n",
    "X = auto.drop(['year', 'origin', 'name'], axis=1)\n",
    "y = auto.origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaAuto = LinearDiscriminantAnalysis()\n",
    "LDAscores = cross_validate(ldaAuto, X, y, cv=5, return_train_score=True)\n",
    "print('Trainingsfehler:', 1-LDAscores['train_score'].mean())\n",
    "print('Testfehler:', 1-LDAscores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNAuto = KNeighborsClassifier(n_neighbors=5)\n",
    "KNNscores = cross_validate(KNNAuto, X.values, y.values, cv=5, return_train_score=True)\n",
    "print('Trainingsfehler:', 1-KNNscores['train_score'].mean())\n",
    "print('Testfehler:', 1-KNNscores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits, random_state=1, shuffle=True)\n",
    "CVFoldTestErrors = []\n",
    "CVFoldTrainErrors = []\n",
    "for trainIndex, testIndex in kf.split(X):\n",
    "    XTrain, XTest = X.iloc[trainIndex], X.iloc[testIndex]\n",
    "    yTrain, yTest = y.iloc[trainIndex], y.iloc[testIndex]\n",
    "    LogAuto = sm.MNLogit(yTrain, XTrain)\n",
    "    LogAutoRes = LogAuto.fit()\n",
    "    testError = 1 - accuracy_score(yTest, LogAutoRes.predict(XTest).idxmax(axis=1) + 1)\n",
    "    trainError = 1 - accuracy_score(yTrain, LogAutoRes.predict(XTrain).idxmax(axis=1) + 1)\n",
    "    CVFoldTrainErrors.append(trainError)\n",
    "    CVFoldTestErrors.append(testError)\n",
    "#print('CV Trainingsfehler:\\n', CVFoldTrainErrors)\n",
    "print('CV Durchschnittlicher Trainingsfehler:', np.mean(CVFoldTrainErrors))\n",
    "#print('CV Testfehler:\\n', CVFoldTestErrors)\n",
    "print('CV Durchschnittlicher Testfehler:', np.mean(CVFoldTestErrors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogAutoRes.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehen Sie sich die Unterschiede zwischen den Ergebnissen mit LDA und dem Validierungsdatensatz etwas genauer an. Wieso kommt es zu den Unterschieden? (Tipp: Eine Ansatz wäre es sich die Confusion-Matrix für die einzelnen Splits anzusehen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialisiere Dataframe mit Vorhersagen\n",
    "yPred = pd.DataFrame({'y': y, 'yPredTr0': 0})\n",
    "# Wiederhole Berechnung auf 10 verschiedenen Random Seeds\n",
    "for i in range(10):\n",
    "    # Split in Train und Validierungsset\n",
    "    XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    # Initalisiere LDA Klassifikator\n",
    "    ldaAuto = LinearDiscriminantAnalysis()\n",
    "    # Trainier LDA Klassifikator\n",
    "    ldaAuto.fit(XTrain, yTrain)\n",
    "    # Berechne Conusion Matrix\n",
    "    confMat = confusion_matrix(yTest, ldaAuto.predict(XTest))\n",
    "    # Gebe Random State aus\n",
    "    print('Random State:', i)\n",
    "    # Gebe Trainingsfehler aus\n",
    "    print('Trainingsfehler:', 1-ldaAuto.score(XTrain, yTrain))\n",
    "    # Gebe Testfehler aus\n",
    "    print('Testfehler:', 1-ldaAuto.score(XTest, yTest))\n",
    "    # Plotte absolute Häufigkeiten der Observationen im Testset\n",
    "    yTest.value_counts().sort_index().plot.bar()\n",
    "    plt.title('Observationen pro Klasse für Testset')\n",
    "    plt.show()\n",
    "    # Berechne Confusionmatrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=confMat,\n",
    "                                  display_labels=ldaAuto.classes_)\n",
    "    # Plotte Confusionmatrix\n",
    "    disp.plot()\n",
    "    plt.title('Confusion Matrix für Testset')\n",
    "    plt.show()\n",
    "    # Neue Zeile in Ausagabe\n",
    "    print('\\n')\n",
    "    # Fülle Dataframe mit Vorhersagen\n",
    "    colTrain = 'yPredTr' + str(i)\n",
    "    yPred.loc[XTrain.index, colTrain] = ldaAuto.predict(XTrain)\n",
    "    colTest = 'yPredTe' + str(i)\n",
    "    yPred.loc[XTest.index, colTest] = ldaAuto.predict(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prüfe welche Observationen im Training bzw. Test richtig zugeordnet wurden\n",
    "yPredBool = yPred.drop('y', axis=1) == np.resize(y, (yPred.shape[0],1))\n",
    "# Berechne wie oft die Observation in den 10 Versuchen richtig klassifiziert wurde\n",
    "yPredTrue = yPredBool.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anteil der Observationen, welche nie richtig klassifiziert wurden, am aktuellen Testset - \"schwierige Observationen\"\n",
    "len(yPredTrue[yPredTrue == 0].index.intersection(yTest.index)) / yTest.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # Split in Train und Validierungsset\n",
    "    XTrain, XTest, yTrain, yTest = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    # Initalisiere LDA Klassifikator\n",
    "    ldaAuto = LinearDiscriminantAnalysis()\n",
    "    # Trainier LDA Klassifikator\n",
    "    ldaAuto.fit(XTrain, yTrain)\n",
    "    # Berechne Conusion Matrix\n",
    "    confMat = confusion_matrix(yTest, ldaAuto.predict(XTest))\n",
    "    # Gebe Random State aus\n",
    "    print('Random State:', i)\n",
    "    # Gebe Trainingsfehler aus\n",
    "    print('Trainingsfehler:', 1-ldaAuto.score(XTrain, yTrain))\n",
    "    # Gebe Testfehler aus\n",
    "    print('Testfehler:', 1-ldaAuto.score(XTest, yTest))\n",
    "    # Gebe Anteil der schiwerig zu klassifierenden Observationen aus\n",
    "    print('Anteil schwierige Observationen:', len(yPredTrue[yPredTrue == 0].index.intersection(yTest.index)) / yTest.shape[0])\n",
    "    # Neue Zeile in Ausagabe\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "3dc50e175bdcbea8a6268d2a01f91a56f3474fd104144995cb29f75edd2d3efd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
